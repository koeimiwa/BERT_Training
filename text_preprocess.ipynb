{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "text_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwgy5lmNYo5"
      },
      "source": [
        "# git フォルダごと削除用\n",
        "import shutil\n",
        "import os\n",
        "shutil.rmtree('./wikiextractor/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLARYHBzEbVP"
      },
      "source": [
        "日本語wikiからコーパスを作成するスクリプトです.<br>\n",
        "https://dumps.wikimedia.org/jawiki/latest/ <br>\n",
        "こちらのサイトから最新版の\"pages-articles\"のアドレスを手に入れてください. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvpOPEueIzGM"
      },
      "source": [
        "# MeCabのインストール\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hkASRrDEbVT"
      },
      "source": [
        "! wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlPJOlayEbVV"
      },
      "source": [
        "ダンプデータには不要なマークアップなどが含まれているので、取り除くためのテキストクリーニング用のスクリプトをgitから持ってきます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa_aF_TLEbVW"
      },
      "source": [
        "# pip でwikiextractorはインストールする\n",
        "!pip install wikiextractor\n",
        "#! git clone https://github.com/attardi/wikiextractor.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnmRRhpEbVX"
      },
      "source": [
        "日本語wikiに対してテキストクリーニングを実行します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZGv97HjEbVY"
      },
      "source": [
        "!python -m wikiextractor.WikiExtractor -o extracted jawiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMDMOXkSEbVZ"
      },
      "source": [
        "テキストに前処理を加えた上で,複数のtxtファイルをひとつに結合します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCPGsE5XEbVb"
      },
      "source": [
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('./tmp.txt','w') as f:\n",
        "    for directory in glob.glob('./extracted/*'):\n",
        "        for name in glob.glob(directory+'/*'):\n",
        "            with open(name, 'r') as r:\n",
        "                for line in r:\n",
        "                    # titleを削除する\n",
        "                    if '<doc ' in line:\n",
        "                        next(r)\n",
        "                        next(r)\n",
        "                    elif '</doc>' in line:\n",
        "                        f.write('\\n')\n",
        "                        continue\n",
        "                    else:\n",
        "                        # 空白・改行削除、大文字を小文字に変換\n",
        "                        text = BeautifulSoup(line.strip()).text.lower()\n",
        "                        f.write(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4BLmzsEbVd"
      },
      "source": [
        "ここからはBERTのトレーニング用にテキストファイルを整形していきます.<br>\n",
        "文章を単語ごとに分割し, ひとつの単元の中に偶数個の文章が含まれるように調整します."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWr2b-o7EbVf"
      },
      "source": [
        "import linecache\n",
        "import random\n",
        "import MeCab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDzHNXzcEbVg"
      },
      "source": [
        "random.seed(42)\n",
        "filename = 'tmp.txt'\n",
        "save_file = 'even_rows1G.txt'\n",
        "LIMIT_BYTE = 1000000000 # 1Gbyte\n",
        "#save_file = 'even_rows100M.txt'\n",
        "#LIMIT_BYTE = 100000000 # 100Mbyte\n",
        "t = MeCab.Tagger('-Owakati')\n",
        "\n",
        "def get_byte_num(s):\n",
        "    return len(s.encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g6r-qEZEbVg"
      },
      "source": [
        "with open(save_file, 'w') as f:\n",
        "    count_byte = 0\n",
        "    with open(filename) as r:\n",
        "        for text in r:\n",
        "            print('{} bytes'.format(count_byte))\n",
        "            text = t.parse(text).strip()\n",
        "            # 一文ごとに分割する\n",
        "            text = text.split('。')\n",
        "            # 空白要素は捨てる\n",
        "            text = [t.strip() for t in text if t]\n",
        "            # 一単元の文書が偶数個の文章から成るようにする(BERTのデータセットの都合上)\n",
        "            max_text_len = len(text) // 2\n",
        "            text = text[:max_text_len * 2]\n",
        "            text = '\\n'.join(text)\n",
        "            f.write(text)\n",
        "            count_byte += get_byte_num(text)\n",
        "            if count_byte >= LIMIT_BYTE:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KFA73WkEbVh"
      },
      "source": [
        "これでBERTの学習に使うデータセットができました.<br>\n",
        "今度はTraining用とValidation用のデータに分割します."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOFwt6fEbVi"
      },
      "source": [
        "num_lines = sum(1 for line in open(save_file))\n",
        "print('Base file lines : ', num_lines)\n",
        "# 全体の80%をTraining dataに当てます\n",
        "train_lines = int(num_lines * 0.8)\n",
        "print('Train file lines : ', train_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wPH_go1EbVj"
      },
      "source": [
        "dataは前処理済みテキスト保存場所 <br>\n",
        "outputは訓練モデル保存場所として作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QKXnyDqEbVj"
      },
      "source": [
        "! mkdir -p data output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCPL7RNgEbVj"
      },
      "source": [
        "out_file_name_temp = './data/splitted_%d.txt'\n",
        "\n",
        "split_index = 1\n",
        "line_index = 1\n",
        "out_file = open(out_file_name_temp % (split_index,), 'w')\n",
        "in_file = open(save_file)\n",
        "line = in_file.readline()\n",
        "while line:\n",
        "    if line_index > train_lines:\n",
        "        print('Starting file: %d' % split_index)\n",
        "        out_file.close()\n",
        "        split_index = split_index + 1\n",
        "        line_index = 1\n",
        "        out_file = open(out_file_name_temp % (split_index,), 'w')\n",
        "    out_file.write(line)\n",
        "    line_index = line_index + 1\n",
        "    line = in_file.readline()\n",
        "    \n",
        "out_file.close()\n",
        "in_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmcjd0xsEbVk"
      },
      "source": [
        "print('Train file lines : ', sum(1 for line in open('./data/splitted_1.txt')))\n",
        "print('Valid file lines : ', sum(1 for line in open('./data/splitted_2.txt')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqo8Yr4dEbVk"
      },
      "source": [
        "これにてテキストの前処理は完了です！"
      ]
    }
  ]
}